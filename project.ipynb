{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d103bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.13/site-packages (0.4.6)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.13/site-packages (6.33.1)\n",
      "Requirement already satisfied: sacrebleu in ./.venv/lib/python3.13/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.13/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.13/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.venv/lib/python3.13/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.13/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.13/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.13/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.venv/lib/python3.13/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.13/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anugrah-singh/Desktop/hinglishly-tanya/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install transformers datasets torch pandas scikit-learn evaluate accelerate sentencepiece protobuf sacrebleu\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorWithPadding, \n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bdc775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2993, 5)\n",
      "Val shape: (1390, 5)\n",
      "Test shape: (6513, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_query</th>\n",
       "      <th>cs_query</th>\n",
       "      <th>en_parse</th>\n",
       "      <th>cs_parse</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Add a new weekly reminder for Sunday Brunch at...</td>\n",
       "      <td>9 : 30 am ko Sunday Brunch ke liye ek naya wee...</td>\n",
       "      <td>[IN:CREATE_ALARM Add a new [SL:PERIOD weekly ]...</td>\n",
       "      <td>[IN:CREATE_ALARM [SL:DATE_TIME 9 : 30 am ko ] ...</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>message danny and see if he wants to go to com...</td>\n",
       "      <td>danny ko message karo aur dekho ke he wants to...</td>\n",
       "      <td>[IN:SEND_MESSAGE message [SL:RECIPIENT danny ]...</td>\n",
       "      <td>[IN:SEND_MESSAGE [SL:RECIPIENT danny ] ko mess...</td>\n",
       "      <td>messaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>set alarm for 2 hours</td>\n",
       "      <td>do ghante ke liye alarm set kardo</td>\n",
       "      <td>[IN:CREATE_ALARM set alarm [SL:DATE_TIME for 2...</td>\n",
       "      <td>[IN:CREATE_ALARM [SL:DATE_TIME do ghante ke li...</td>\n",
       "      <td>alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kill the reminder for baking a cake for neil</td>\n",
       "      <td>neil ke liye cake bake karne ke reminder ko mi...</td>\n",
       "      <td>[IN:DELETE_REMINDER kill the reminder for [SL:...</td>\n",
       "      <td>[IN:DELETE_REMINDER [SL:TODO neil ke liye cake...</td>\n",
       "      <td>reminder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retrieve my chat requests please</td>\n",
       "      <td>Please mere chat requests ko retrieve kare</td>\n",
       "      <td>[IN:GET_MESSAGE retrieve my chat requests plea...</td>\n",
       "      <td>[IN:GET_MESSAGE Please mere chat requests ko r...</td>\n",
       "      <td>messaging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            en_query  \\\n",
       "0  Add a new weekly reminder for Sunday Brunch at...   \n",
       "1  message danny and see if he wants to go to com...   \n",
       "2                              set alarm for 2 hours   \n",
       "3       kill the reminder for baking a cake for neil   \n",
       "4                   retrieve my chat requests please   \n",
       "\n",
       "                                            cs_query  \\\n",
       "0  9 : 30 am ko Sunday Brunch ke liye ek naya wee...   \n",
       "1  danny ko message karo aur dekho ke he wants to...   \n",
       "2                  do ghante ke liye alarm set kardo   \n",
       "3  neil ke liye cake bake karne ke reminder ko mi...   \n",
       "4         Please mere chat requests ko retrieve kare   \n",
       "\n",
       "                                            en_parse  \\\n",
       "0  [IN:CREATE_ALARM Add a new [SL:PERIOD weekly ]...   \n",
       "1  [IN:SEND_MESSAGE message [SL:RECIPIENT danny ]...   \n",
       "2  [IN:CREATE_ALARM set alarm [SL:DATE_TIME for 2...   \n",
       "3  [IN:DELETE_REMINDER kill the reminder for [SL:...   \n",
       "4  [IN:GET_MESSAGE retrieve my chat requests plea...   \n",
       "\n",
       "                                            cs_parse     domain  \n",
       "0  [IN:CREATE_ALARM [SL:DATE_TIME 9 : 30 am ko ] ...      alarm  \n",
       "1  [IN:SEND_MESSAGE [SL:RECIPIENT danny ] ko mess...  messaging  \n",
       "2  [IN:CREATE_ALARM [SL:DATE_TIME do ghante ke li...      alarm  \n",
       "3  [IN:DELETE_REMINDER [SL:TODO neil ke liye cake...   reminder  \n",
       "4  [IN:GET_MESSAGE Please mere chat requests ko r...  messaging  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "data_dir = \"data\"\n",
    "train_df = pd.read_caddsv(os.path.join(data_dir, \"train.tsv\"), sep=\"\\t\")\n",
    "val_df = pd.read_csv(os.path.join(data_dir, \"validation.tsv\"), sep=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"test.tsv\"), sep=\"\\t\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Val shape: {val_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Display sample\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bdbf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic data (this may take a moment)...\n",
      "New Synthetic Distribution:\n",
      "error_type\n",
      "hinglish_detection       2993\n",
      "spelling_error           2993\n",
      "grammar_error            2993\n",
      "punctuation_error        2993\n",
      "word_choice_error        2993\n",
      "needs_rewrite            2993\n",
      "slang_or_informal        2993\n",
      "transliteration_error    2993\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Labels: ['grammar_error' 'hinglish_detection' 'needs_rewrite' 'punctuation_error'\n",
      " 'slang_or_informal' 'spelling_error' 'transliteration_error'\n",
      " 'word_choice_error']\n",
      "New Synthetic Distribution:\n",
      "error_type\n",
      "hinglish_detection       2993\n",
      "spelling_error           2993\n",
      "grammar_error            2993\n",
      "punctuation_error        2993\n",
      "word_choice_error        2993\n",
      "needs_rewrite            2993\n",
      "slang_or_informal        2993\n",
      "transliteration_error    2993\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Labels: ['grammar_error' 'hinglish_detection' 'needs_rewrite' 'punctuation_error'\n",
      " 'slang_or_informal' 'spelling_error' 'transliteration_error'\n",
      " 'word_choice_error']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23944/23944 [00:01<00:00, 22812.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23944/23944 [00:01<00:00, 22812.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11120/11120 [00:00<00:00, 23760.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11120/11120 [00:00<00:00, 23760.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52104/52104 [00:02<00:00, 21985.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52104/52104 [00:02<00:00, 21985.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Task 1: Error Detection (BERT) ---\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Define Augmentation Functions to create Synthetic Data\n",
    "def introduce_spelling_error(text):\n",
    "    words = text.split()\n",
    "    if not words: return text\n",
    "    idx = random.randint(0, len(words) - 1)\n",
    "    word = list(words[idx])\n",
    "    if len(word) > 1:\n",
    "        i = random.randint(0, len(word) - 2)\n",
    "        word[i], word[i+1] = word[i+1], word[i]\n",
    "        words[idx] = \"\".join(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def introduce_grammar_error(text):\n",
    "    words = text.split()\n",
    "    if len(words) > 1:\n",
    "        words.pop(random.randint(0, len(words) - 1))\n",
    "    return \" \".join(words)\n",
    "\n",
    "def introduce_punctuation_error(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def introduce_word_choice_error(text):\n",
    "    words = text.split()\n",
    "    if not words: return text\n",
    "    idx = random.randint(0, len(words) - 1)\n",
    "    words.insert(idx, words[idx])\n",
    "    return \" \".join(words)\n",
    "\n",
    "def introduce_needs_rewrite(text):\n",
    "    words = text.split()\n",
    "    if len(words) > 1:\n",
    "        random.shuffle(words)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def create_synthetic_dataset(df):\n",
    "    new_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        base_text = str(row[\"cs_query\"])\n",
    "        target_text = str(row[\"en_query\"])\n",
    "        \n",
    "        # 1. Hinglish Detection (Clean)\n",
    "        new_rows.append({\"cs_query\": base_text, \"en_query\": target_text, \"error_type\": \"hinglish_detection\"})\n",
    "        \n",
    "        # 2. Spelling Error\n",
    "        new_rows.append({\"cs_query\": introduce_spelling_error(base_text), \"en_query\": target_text, \"error_type\": \"spelling_error\"})\n",
    "        \n",
    "        # 3. Grammar Error\n",
    "        new_rows.append({\"cs_query\": introduce_grammar_error(base_text), \"en_query\": target_text, \"error_type\": \"grammar_error\"})\n",
    "        \n",
    "        # 4. Punctuation Error\n",
    "        new_rows.append({\"cs_query\": introduce_punctuation_error(base_text), \"en_query\": target_text, \"error_type\": \"punctuation_error\"})\n",
    "        \n",
    "        # 5. Word Choice Error\n",
    "        new_rows.append({\"cs_query\": introduce_word_choice_error(base_text), \"en_query\": target_text, \"error_type\": \"word_choice_error\"})\n",
    "        \n",
    "        # 6. Needs Rewrite\n",
    "        new_rows.append({\"cs_query\": introduce_needs_rewrite(base_text), \"en_query\": target_text, \"error_type\": \"needs_rewrite\"})\n",
    "        \n",
    "        # 7. Slang (Simulated)\n",
    "        slang_text = base_text.lower().replace(\"ing\", \"in\").replace(\"you\", \"u\").replace(\"hai\", \"h\")\n",
    "        new_rows.append({\"cs_query\": slang_text, \"en_query\": target_text, \"error_type\": \"slang_or_informal\"})\n",
    "        \n",
    "        # 8. Transliteration (Simulated)\n",
    "        trans_text = base_text.replace(\"ee\", \"i\").replace(\"oo\", \"u\").replace(\"aa\", \"a\")\n",
    "        new_rows.append({\"cs_query\": trans_text, \"en_query\": target_text, \"error_type\": \"transliteration_error\"})\n",
    "\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "# Generate Synthetic Data\n",
    "print(\"Generating synthetic data (this may take a moment)...\")\n",
    "train_df = create_synthetic_dataset(train_df)\n",
    "val_df = create_synthetic_dataset(val_df)\n",
    "test_df = create_synthetic_dataset(test_df)\n",
    "\n",
    "print(\"New Synthetic Distribution:\")\n",
    "print(train_df[\"error_type\"].value_counts())\n",
    "\n",
    "# Encode Labels\n",
    "le = LabelEncoder()\n",
    "train_df[\"label\"] = le.fit_transform(train_df[\"error_type\"])\n",
    "val_df[\"label\"] = le.transform(val_df[\"error_type\"])\n",
    "test_df[\"label\"] = le.transform(test_df[\"error_type\"])\n",
    "\n",
    "label_list = le.classes_\n",
    "num_labels = len(label_list)\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "print(f\"\\nLabels: {label_list}\")\n",
    "\n",
    "# Create HF Datasets\n",
    "hf_train = Dataset.from_pandas(train_df[[\"cs_query\", \"label\"]])\n",
    "hf_val = Dataset.from_pandas(val_df[[\"cs_query\", \"label\"]])\n",
    "hf_test = Dataset.from_pandas(test_df[[\"cs_query\", \"label\"]])\n",
    "\n",
    "# Tokenization\n",
    "model_checkpoint_cls = \"bert-base-multilingual-cased\"\n",
    "tokenizer_cls = AutoTokenizer.from_pretrained(model_checkpoint_cls)\n",
    "\n",
    "def preprocess_function_cls(examples):\n",
    "    return tokenizer_cls(examples[\"cs_query\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_train_cls = hf_train.map(preprocess_function_cls, batched=True)\n",
    "tokenized_val_cls = hf_val.map(preprocess_function_cls, batched=True)\n",
    "tokenized_test_cls = hf_test.map(preprocess_function_cls, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a41370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_8457/2920293106.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_cls = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4491' max='4491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4491/4491 34:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.663400</td>\n",
       "      <td>1.315541</td>\n",
       "      <td>0.496403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.157600</td>\n",
       "      <td>1.104510</td>\n",
       "      <td>0.568435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.962200</td>\n",
       "      <td>1.059104</td>\n",
       "      <td>0.586511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3257' max='3257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3257/3257 06:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation: {'eval_loss': 1.08975088596344, 'eval_accuracy': 0.5859434976201443, 'eval_runtime': 372.5984, 'eval_samples_per_second': 139.84, 'eval_steps_per_second': 8.741, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "model_cls = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint_cls, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics_cls(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Training Arguments\n",
    "training_args_cls = TrainingArguments(\n",
    "    output_dir=\"bert-domain-classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer_cls = Trainer(\n",
    "    model=model_cls,\n",
    "    args=training_args_cls,\n",
    "    train_dataset=tokenized_train_cls,\n",
    "    eval_dataset=tokenized_val_cls,\n",
    "    tokenizer=tokenizer_cls,\n",
    "    compute_metrics=compute_metrics_cls,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer_cls.train()\n",
    "\n",
    "# Evaluate\n",
    "eval_results_cls = trainer_cls.evaluate(tokenized_test_cls)\n",
    "print(f\"BERT Evaluation: {eval_results_cls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anugrah-singh/Desktop/hinglishly-tanya/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/23944 [00:00<?, ? examples/s]/home/anugrah-singh/Desktop/hinglishly-tanya/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 8000/23944 [00:00<00:00, 58779.15 examples/s]/home/anugrah-singh/Desktop/hinglishly-tanya/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23944/23944 [00:00<00:00, 59912.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23944/23944 [00:00<00:00, 59912.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11120/11120 [00:00<00:00, 67714.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11120/11120 [00:00<00:00, 67714.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52104/52104 [00:00<00:00, 52465.89 examples/s]\n",
      "\n",
      "/tmp/ipykernel_46089/3888454677.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_seq = Seq2SeqTrainer(\n",
      "/tmp/ipykernel_46089/3888454677.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_seq = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1498' max='4491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1498/4491 41:08 < 1:22:18, 0.61 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7751' max='11120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7751/11120 38:36 < 16:47, 3.35 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Task 2: Grammar Correction / Translation (Seq2Seq) ---\n",
    "# We will train the model to take Hinglish (cs_query) and output corrected English (en_query).\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear memory from previous tasks\n",
    "for name in ['model_cls', 'trainer_cls', 'model_seq', 'trainer_seq']:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Create HF Datasets\n",
    "hf_train_seq = Dataset.from_pandas(train_df[[\"en_query\", \"cs_query\"]])\n",
    "hf_val_seq = Dataset.from_pandas(val_df[[\"en_query\", \"cs_query\"]])\n",
    "hf_test_seq = Dataset.from_pandas(test_df[[\"en_query\", \"cs_query\"]])\n",
    "\n",
    "# Tokenization\n",
    "model_checkpoint_seq = \"google/mt5-small\"\n",
    "tokenizer_seq = AutoTokenizer.from_pretrained(model_checkpoint_seq)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function_seq(examples):\n",
    "    inputs = examples[\"cs_query\"] # Input: Hinglish\n",
    "    targets = examples[\"en_query\"] # Target: Corrected English\n",
    "    model_inputs = tokenizer_seq(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    with tokenizer_seq.as_target_tokenizer():\n",
    "        labels = tokenizer_seq(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_seq = hf_train_seq.map(preprocess_function_seq, batched=True)\n",
    "tokenized_val_seq = hf_val_seq.map(preprocess_function_seq, batched=True)\n",
    "tokenized_test_seq = hf_test_seq.map(preprocess_function_seq, batched=True)\n",
    "\n",
    "# Load Model\n",
    "model_seq = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint_seq)\n",
    "\n",
    "# Optimization for Low VRAM (6GB)\n",
    "model_seq.config.use_cache = False # Required for gradient checkpointing\n",
    "\n",
    "# Data Collator\n",
    "data_collator_seq = DataCollatorForSeq2Seq(tokenizer_seq, model=model_seq)\n",
    "\n",
    "# Metrics\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics_seq(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer_seq.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer_seq.pad_token_id)\n",
    "    decoded_labels = tokenizer_seq.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    return result\n",
    "\n",
    "# Training Arguments\n",
    "training_args_seq = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mt5-hinglish-correction\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1, # Lowest possible batch size\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16, # Accumulate gradients to simulate batch size 16\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=torch.cuda.is_available(), # Use Mixed Precision (saves memory)\n",
    "    gradient_checkpointing=True,    # Trade compute for memory\n",
    "    optim=\"adafactor\",              # Use Adafactor optimizer (less memory than AdamW)\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer_seq = Seq2SeqTrainer(\n",
    "    model=model_seq,\n",
    "    args=training_args_seq,\n",
    "    train_dataset=tokenized_train_seq,\n",
    "    eval_dataset=tokenized_val_seq,\n",
    "    tokenizer=tokenizer_seq,\n",
    "    data_collator=data_collator_seq,\n",
    "    compute_metrics=compute_metrics_seq,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer_seq.train()\n",
    "\n",
    "# Re-enable cache for evaluation\n",
    "model_seq.config.use_cache = True\n",
    "\n",
    "# Evaluate\n",
    "eval_results_seq = trainer_seq.evaluate(tokenized_test_seq)\n",
    "print(f\"Correction Evaluation: {eval_results_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8086c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning up GPU memory...\n",
      "ðŸ’¾ GPU Memory Free: 5.67 GB\n",
      "âœ… Resuming from: mt5-hinglish-correction/checkpoint-1497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/anugrah-singh/Desktop/hinglishly-tanya/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/home/anugrah-singh/Desktop/hinglishly-tanya/.venv/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/23944 [00:00<?, ? examples/s]/home/anugrah-singh/Desktop/hinglishly-tanya/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map:  25%|â–ˆâ–ˆâ–Œ       | 6000/23944 [00:00<00:00, 55118.47 examples/s]/home/anugrah-singh/Desktop/hinglishly-tanya/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23944/23944 [00:00<00:00, 55724.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23944/23944 [00:00<00:00, 55724.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11120/11120 [00:00<00:00, 54836.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11120/11120 [00:00<00:00, 54836.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52104/52104 [00:01<00:00, 45958.93 examples/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading model from checkpoint...\n",
      "ðŸ”§ Using BF16: True\n",
      "\n",
      "ðŸš€ Starting training from mt5-hinglish-correction/checkpoint-1497\n",
      "ðŸ“Š Settings: batch_size=1, grad_accum=16 (effective batch=16)\n",
      "ðŸŽ¯ Target: 6 epochs\n",
      "\n",
      "ðŸ”§ Using BF16: True\n",
      "\n",
      "ðŸš€ Starting training from mt5-hinglish-correction/checkpoint-1497\n",
      "ðŸ“Š Settings: batch_size=1, grad_accum=16 (effective batch=16)\n",
      "ðŸŽ¯ Target: 6 epochs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10390/706418509.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_seq = Seq2SeqTrainer(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Could not resume from checkpoint: 'NoneType' object has no attribute 'load_state_dict'\n",
      "ðŸ”„ Starting training from the loaded checkpoint weights instead...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1002' max='8982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1002/8982 41:47 < 5:33:28, 0.40 it/s, Epoch 0.67/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === RESUME TRAINING FROM CHECKPOINT (Optimized for 6GB GPU) ===\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Step 1: Clean up GPU memory\n",
    "print(\"ðŸ§¹ Cleaning up GPU memory...\")\n",
    "if 'model_seq' in globals():\n",
    "    del model_seq\n",
    "if 'trainer_seq' in globals():\n",
    "    del trainer_seq\n",
    "if 'model_cls' in globals():\n",
    "    del model_cls\n",
    "if 'trainer_cls' in globals():\n",
    "    del trainer_cls\n",
    "    \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(f\"ðŸ’¾ GPU Memory Free: {torch.cuda.get_device_properties(0).total_memory / 1024**3 - torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Step 2: Find the latest checkpoint\n",
    "checkpoint_dir = \"mt5-hinglish-correction\"\n",
    "checkpoints = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "if checkpoints:\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    print(f\"âœ… Resuming from: {checkpoint_path}\")\n",
    "else:\n",
    "    checkpoint_path = None\n",
    "    print(\"âŒ No checkpoint found!\")\n",
    "\n",
    "# Step 3: Prepare tokenizer and datasets\n",
    "model_checkpoint_seq = \"google/mt5-small\"\n",
    "tokenizer_seq = AutoTokenizer.from_pretrained(model_checkpoint_seq)\n",
    "\n",
    "def preprocess_function_seq(examples):\n",
    "    inputs = examples[\"cs_query\"]\n",
    "    targets = examples[\"en_query\"]\n",
    "    model_inputs = tokenizer_seq(inputs, max_length=128, truncation=True)\n",
    "    with tokenizer_seq.as_target_tokenizer():\n",
    "        labels = tokenizer_seq(targets, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Recreate datasets\n",
    "hf_train_seq = Dataset.from_pandas(train_df[[\"en_query\", \"cs_query\"]])\n",
    "hf_val_seq = Dataset.from_pandas(val_df[[\"en_query\", \"cs_query\"]])\n",
    "hf_test_seq = Dataset.from_pandas(test_df[[\"en_query\", \"cs_query\"]])\n",
    "\n",
    "tokenized_train_seq = hf_train_seq.map(preprocess_function_seq, batched=True)\n",
    "tokenized_val_seq = hf_val_seq.map(preprocess_function_seq, batched=True)\n",
    "tokenized_test_seq = hf_test_seq.map(preprocess_function_seq, batched=True)\n",
    "\n",
    "# Step 4: Load model from checkpoint - use FP32 to avoid FP16 gradient issues\n",
    "print(f\"ðŸ“¥ Loading model from checkpoint...\")\n",
    "model_seq = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "model_seq.config.use_cache = False\n",
    "\n",
    "# Step 5: Setup training components\n",
    "data_collator_seq = DataCollatorForSeq2Seq(tokenizer_seq, model=model_seq)\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics_seq(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer_seq.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer_seq.pad_token_id)\n",
    "    decoded_labels = tokenizer_seq.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "# Check if BF16 is available (better than FP16 for training)\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "print(f\"ðŸ”§ Using BF16: {use_bf16}\")\n",
    "\n",
    "# Ultra-aggressive settings for 6GB GPU\n",
    "training_args_seq = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mt5-hinglish-correction\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,  # Match original training\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,  # Keep only 1 checkpoint to save space\n",
    "    num_train_epochs=6,  # Continue to 6 epochs\n",
    "    predict_with_generate=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,  # Disable to save memory\n",
    "    push_to_hub=False,\n",
    "    bf16=use_bf16,  # Use BF16 if available\n",
    "    fp16=False,  # Disable FP16 to avoid gradient scaling issues\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    max_grad_norm=1.0,\n",
    "    dataloader_num_workers=0,\n",
    "    logging_steps=100,\n",
    "    eval_accumulation_steps=1,\n",
    "    generation_max_length=128,\n",
    ")\n",
    "\n",
    "trainer_seq = Seq2SeqTrainer(\n",
    "    model=model_seq,\n",
    "    args=training_args_seq,\n",
    "    train_dataset=tokenized_train_seq,\n",
    "    eval_dataset=tokenized_val_seq,\n",
    "    tokenizer=tokenizer_seq,\n",
    "    data_collator=data_collator_seq,\n",
    "    compute_metrics=compute_metrics_seq,\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸš€ Starting training from {checkpoint_path}\")\n",
    "print(f\"ðŸ“Š Settings: batch_size=1, grad_accum=16 (effective batch=16)\")\n",
    "print(f\"ðŸŽ¯ Target: {training_args_seq.num_train_epochs} epochs\\n\")\n",
    "\n",
    "# Step 6: Resume training - start fresh if checkpoint resume fails\n",
    "try:\n",
    "    trainer_seq.train(resume_from_checkpoint=checkpoint_path)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not resume from checkpoint: {e}\")\n",
    "    print(\"ðŸ”„ Starting training from the loaded checkpoint weights instead...\")\n",
    "    trainer_seq.train()\n",
    "\n",
    "# Step 7: Final evaluation\n",
    "model_seq.config.use_cache = True\n",
    "print(\"\\nðŸ§ª Evaluating on test set...\")\n",
    "eval_results = trainer_seq.evaluate(tokenized_test_seq)\n",
    "print(f\"\\nâœ… Final BLEU Score: {eval_results['eval_bleu']:.2f}\")\n",
    "print(f\"ðŸ“ˆ Full Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947a3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
